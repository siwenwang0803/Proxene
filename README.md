# Proxene â€“ Developer-First AI Governance Proxy

**åªéœ€æŠŠ LLM è¯·æ±‚æŒ‡å‘ Proxeneï¼Œå³åˆ»çœæˆæœ¬ã€å»æ•ã€åˆè§„å®¡è®¡ï¼Œå…¨éƒ¨ç”¨ YAML/VS-Code è°ƒè¯•ã€‚**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)

## ğŸš€ Quick Start

```bash
# Install
pip install proxene

# Start proxy
proxene start --port 8080

# Point your LLM calls to Proxene
# Old: https://api.openai.com/v1/chat/completions
# New: http://localhost:8080/v1/chat/completions
```

## âœ¨ Features

- **Policy-as-Code** - Define cost limits, PII detection, and routing rules in YAML
- **Cost Guard** - Real-time token counting with per-request/minute/daily caps
- **PII Shield** - Auto-detect and redact sensitive data (SSN, emails, credit cards)
- **Local-First** - Debug policies with CLI replay, no cloud dependency
- **VS Code Integration** - Live request monitoring and policy debugging

## ğŸ“‹ Example Policy

```yaml
# policies/default.yaml
cost_limits:
  max_per_request: 0.03
  daily_cap: 100.00

pii_detection:
  enabled: true
  action: redact  # or: block, warn

routing:
  - if: complexity < 3
    use: claude-3-haiku
  - else:
    use: gpt-4o
```

## ğŸ› ï¸ Architecture

```
Your App â†’ Proxene Proxy â†’ LLM Provider
             â†“
         [Policy Engine]
         [Cost Guard]
         [PII Detector]
         [OTEL Logging]
```

## ğŸ“Š Observability

Built-in OpenTelemetry support for monitoring:
- Request costs and token usage
- PII detection hits
- Policy violations
- Model routing decisions

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md).

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file

---

Built with â¤ï¸ for developers who need simple, powerful AI governance